{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "np.seterr(all=\"ignore\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# df = pd.read_csv(\"data/heart_2020_cleaned.csv\")\n",
    "# train, val = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"HeartDisease\"])\n",
    "# val, test = train_test_split(val, test_size=0.5, random_state=42, stratify=val[\"HeartDisease\"])\n",
    "# train.to_csv(\"data/heart_train.csv\", index=False)\n",
    "# val.to_csv(\"data/heart_val.csv\", index=False)\n",
    "# test.to_csv(\"data/heart_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/heart_train.csv')\n",
    "val = pd.read_csv('data/heart_val.csv')\n",
    "test = pd.read_csv('data/heart_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "\n",
    "categorical_features = [\n",
    "    \"Smoking\",\n",
    "    \"AlcoholDrinking\",\n",
    "    \"Stroke\",\n",
    "    \"DiffWalking\",\n",
    "    \"Sex\",\n",
    "    \"AgeCategory\",\n",
    "    \"Race\",\n",
    "    \"Diabetic\",\n",
    "    \"PhysicalActivity\",\n",
    "    \"GenHealth\",\n",
    "    \"Asthma\",\n",
    "    \"KidneyDisease\",\n",
    "    \"SkinCancer\"\n",
    "]\n",
    "\n",
    "continuos_features = [\n",
    "    \"BMI\",\n",
    "    \"PhysicalHealth\",\n",
    "    \"MentalHealth\"\n",
    "]\n",
    "\n",
    "target_variable = \"HeartDisease\"\n",
    "\n",
    "def data_preprocessing(data: pd.DataFrame, categorical_features: List[str], continuous_features: List[str], target_variable: str):\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    # protected variables\n",
    "    sex = df[\"Sex\"].values\n",
    "    age = df[\"AgeCategory\"].values\n",
    "    race = df[\"Race\"].values\n",
    "\n",
    "    # target\n",
    "    target = df[target_variable].values\n",
    "\n",
    "    df_processed = df[categorical_features + continuous_features]\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=False, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    return df_processed, df, target, sex, age, race\n",
    "\n",
    "#df_processed, df_original, target, sex, age, race = data_preprocessing(df, categorical_features=categorical_features, continuous_features=continuos_features, target_variable=target_variable)\n",
    "train_processed, train_original, train_target, train_sex, train_age, train_race = data_preprocessing(train, categorical_features=categorical_features, continuous_features=continuos_features, target_variable=target_variable)\n",
    "val_processed, val_original, val_target, val_sex, val_age, val_race = data_preprocessing(val, categorical_features=categorical_features, continuous_features=continuos_features, target_variable=target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def reproject_features(data: pd.DataFrame, protected_cols: List[str], nonprotected_cols: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    generate a fair representation of nonprotected columns which are independent from any columns in protected_cols\n",
    "    \n",
    "    data : pd.DataFrame\n",
    "        dataframe with columns to be projected\n",
    "    protected_cols : List[str]\n",
    "        list of protected columns\n",
    "    nonprotected_cols : List[str]\n",
    "        list of non-protected columns   \n",
    "    \"\"\"\n",
    "    # make a copy of data\n",
    "    df: pd.DataFrame = data.copy()\n",
    "    ## df is our data\n",
    "    # extract data about protected columns\n",
    "    nonprotect: np.ndarray = df[nonprotected_cols].values\n",
    "    protect: np.ndarray = df[protected_cols].values\n",
    "    # extract data about nonprotected columns\n",
    "    debiased_nonprotect: np.ndarray = df[nonprotected_cols].values\n",
    "    # crease an orthonormal basis\n",
    "    base_protect: np.ndarray = scipy.linalg.orth(protect)\n",
    "\n",
    "    # go through all protected attributes and calculate their contribution to the reprojection to the hyperplane\n",
    "    for j in range(debiased_nonprotect.shape[1]):\n",
    "        debiased_nonprotect[:,j] -= base_protect @ base_protect.T @ nonprotect[:,j]\n",
    "    return debiased_nonprotect\n",
    "\n",
    "def reproject_features_w_regul(data: pd.DataFrame, protected_cols: List[str], nonprotected_cols: List[str], lambda_: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    generate a fair representation of nonprotected columns which are independent from any columns in protected_cols\n",
    "    dat_: a data frame\n",
    "    protected_cols: list of strings, the protected columns\n",
    "    nonprotected_col: string, all other data columns \n",
    "    lambda_: float number between 0 and 1, 0 means totally fair; 1 means same as raw data\n",
    "    \"\"\"\n",
    "    \n",
    "    # run the normal reproject_features function\n",
    "    r: np.ndarray = reproject_features(data, protected_cols, nonprotected_cols)\n",
    "    \n",
    "    # extract data about nonprotected variables\n",
    "    nonprotect: np.ndarray = data[nonprotected_cols].values\n",
    "    # standardize columns\n",
    "\n",
    "    return r + lambda_*(nonprotect - r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just renaming stuff\n",
    "# X_train, X_val = train_processed.values, val_processed.values\n",
    "y_train, y_val = train_target, val_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [No] and group [A]: 0.5 \n",
      "{'A': 0.5}\n",
      "Class differences std: 0.0\n",
      "Target [Yes] and group [B]: 1.0 \n",
      "Target [Yes] and group [C]: 1.0 \n",
      "{'A': 0, 'B': 1.0, 'C': 1.0}\n",
      "Class differences std: 0.4714045207910317\n",
      "Total class difference: 0.4714045207910317\n",
      "Total class difference: 0.4714045207910317\n",
      "(0.4714045207910317, {'No': {'A': 0.5}, 'Yes': {'A': 0, 'B': 1.0, 'C': 1.0}})\n"
     ]
    }
   ],
   "source": [
    "# Fairness metric function\n",
    "import itertools\n",
    "\n",
    "def equailized_odds(preds: np.ndarray, groups: np.ndarray, test: np.ndarray, sum_of_differences: bool = True, verbose: bool = False) -> Union[float, Dict]:\n",
    "    \"\"\"\n",
    "    Calculates the equailized odds of a binary classification problem.\n",
    "    preds: predictions of the model\n",
    "    groups: group labels of the test data\n",
    "    test: test data\n",
    "    sum_of_differences: if True, the sum of the differences is returned, else the mean of the differences is returned\n",
    "    verbose: if True, prints the results\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(list(zip(preds, groups, test)), columns=['preds', 'groups', 'test'])\n",
    "    \n",
    "    # save all results\n",
    "    all_results = {}\n",
    "\n",
    "    total_class_difference = 0\n",
    "    for target in df['test'].unique():\n",
    "        results = {}\n",
    "        for group in df['groups'].unique():\n",
    "            \n",
    "            # get the group and amount of corrects in the group\n",
    "            selection = df.loc[(df['test'] == target) & (df['groups'] == group)]\n",
    "            corrects = selection.loc[selection['preds'] == 'Yes']\n",
    "    \n",
    "            # if there are no corrects in the group, skip\n",
    "            if len(corrects) == 0:\n",
    "                if target == 'Yes':\n",
    "                    results[group] = 0\n",
    "                continue\n",
    "\n",
    "            # get the odds ratio\n",
    "            score = round(len(corrects) / len(selection), 3)\n",
    "\n",
    "            # add the score to the results\n",
    "            results[group] = score\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Target [{target}] and group [{group}]: {score} ')\n",
    "    \n",
    "        class_differences = np.std(list(results.values()))\n",
    "        \n",
    "        if verbose:\n",
    "            print(results)\n",
    "            print(f'Class differences std: {class_differences}')\n",
    "                \n",
    "\n",
    "        # sum up differences or take average\n",
    "        total_class_difference += class_differences\n",
    "\n",
    "        all_results[target] = results\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Total class difference: {total_class_difference}')\n",
    "\n",
    "    print(f'Total class difference: {total_class_difference}')\n",
    "    return total_class_difference, all_results\n",
    "\n",
    "preds = [0,1,1,1,1]\n",
    "preds = ['No', 'Yes', 'Yes', 'Yes', 'Yes']\n",
    "groups = [\"A\", \"A\", \"B\", \"B\", \"C\"]\n",
    "test = [0,0,1,1,1]\n",
    "test = ['No', 'No', 'Yes', 'Yes', 'Yes']\n",
    "\n",
    "print(equailized_odds(preds, groups, test, verbose=True, sum_of_differences=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(1)\n",
    "\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_train: np.ndarray,\n",
    "        X_val: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        y_val: np.ndarray,\n",
    "        group_val: np.ndarray,\n",
    "        evaluation_func: Callable,\n",
    "    ):\n",
    "        self.X_train = X_train\n",
    "        self.X_val = X_val\n",
    "        self.y_train = y_train\n",
    "        self.y_val = y_val\n",
    "        self.group_val = group_val\n",
    "        self.evaluation_func = evaluation_func\n",
    "\n",
    "    def __call__(self, trial) -> float:\n",
    "        \"\"\"This method is called by Optuna to compute the objective\n",
    "        function.\"\"\"\n",
    "        # Initialize general hyper parameters\n",
    "\n",
    "        params = {\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 5, 29, step=2),\n",
    "            #\"min_samples_split\": trial.suggest_loguniform(\"min_samples_split\", 1e-3, 0.01),\n",
    "            \"min_samples_split\": trial.suggest_float(\"min_samples_split\", 1e-5, 0.01),\n",
    "            #\"min_samples_leaf\": trial.suggest_loguniform(\"min_samples_leaf\", 1e-3, 0.01),\n",
    "            \"min_samples_leaf\": trial.suggest_float(\"min_samples_leaf\", 1e-5, 0.01),\n",
    "        }\n",
    "\n",
    "        # parameters for fitting a model\n",
    "        whitebox_model = DecisionTreeClassifier(\n",
    "            **params, random_state=42).fit(self.X_train, self.y_train)\n",
    "\n",
    "        preds: np.ndarray = whitebox_model.predict(self.X_val)\n",
    "\n",
    "        return self.evaluation_func(preds, self.group_val, self.y_val, verbose = True)[0], metrics.f1_score(self.y_val, preds, labels = ['Yes'], pos_label = 'Yes')\n",
    "        #return metrics.f1_score(self.y_val, preds, labels = ['Yes'], pos_label = 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:40,909]\u001b[0m A new study created in memory with name: no-name-4bad823c-959c-4307-8c5a-a2306bf867ad\u001b[0m\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target [No] and group [White]: 0.319 \n",
      "Target [No] and group [Black]: 0.332 \n",
      "Target [No] and group [Other]: 0.265 \n",
      "Target [No] and group [Asian]: 0.176 \n",
      "Target [No] and group [Hispanic]: 0.215 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.338 \n",
      "{'White': 0.319, 'Black': 0.332, 'Other': 0.265, 'Asian': 0.176, 'Hispanic': 0.215, 'American Indian/Alaskan Native': 0.338}\n",
      "Class differences std: 0.06144215888857495\n",
      "Target [Yes] and group [White]: 0.811 \n",
      "Target [Yes] and group [Black]: 0.802 \n",
      "Target [Yes] and group [Other]: 0.766 \n",
      "Target [Yes] and group [Asian]: 0.786 \n",
      "Target [Yes] and group [Hispanic]: 0.696 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.767 \n",
      "{'White': 0.811, 'Black': 0.802, 'Other': 0.766, 'Asian': 0.786, 'Hispanic': 0.696, 'American Indian/Alaskan Native': 0.767}\n",
      "Class differences std: 0.037530728151150466\n",
      "Total class difference: 0.09897288703972543\n",
      "Total class difference: 0.09897288703972543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:16,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:43,099]\u001b[0m Trial 0 finished with values: [0.09897288703972543, 0.31636625811103103] and parameters: {'criterion': 'gini', 'max_depth': 23, 'min_samples_split': 0.0012424539745925075, 'min_samples_leaf': 0.003999971355412032}. \u001b[0m\n",
      "Target [No] and group [White]: 0.321 \n",
      "Target [No] and group [Black]: 0.331 \n",
      "Target [No] and group [Other]: 0.267 \n",
      "Target [No] and group [Asian]: 0.184 \n",
      "Target [No] and group [Hispanic]: 0.22 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.346 \n",
      "{'White': 0.321, 'Black': 0.331, 'Other': 0.267, 'Asian': 0.184, 'Hispanic': 0.22, 'American Indian/Alaskan Native': 0.346}\n",
      "Class differences std: 0.0600039350561463\n",
      "Target [Yes] and group [White]: 0.797 \n",
      "Target [Yes] and group [Black]: 0.814 \n",
      "Target [Yes] and group [Other]: 0.766 \n",
      "Target [Yes] and group [Asian]: 0.786 \n",
      "Target [Yes] and group [Hispanic]: 0.688 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.8 \n",
      "{'White': 0.797, 'Black': 0.814, 'Other': 0.766, 'Asian': 0.786, 'Hispanic': 0.688, 'American Indian/Alaskan Native': 0.8}\n",
      "Class differences std: 0.04163498795751265\n",
      "Total class difference: 0.10163892301365895\n",
      "Total class difference: 0.10163892301365895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:13,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:44,788]\u001b[0m Trial 1 finished with values: [0.10163892301365895, 0.3114030881365685] and parameters: {'criterion': 'entropy', 'max_depth': 21, 'min_samples_split': 0.0013850459199764274, 'min_samples_leaf': 0.0061142762710445905}. \u001b[0m\n",
      "Target [No] and group [White]: 0.306 \n",
      "Target [No] and group [Black]: 0.323 \n",
      "Target [No] and group [Other]: 0.26 \n",
      "Target [No] and group [Asian]: 0.18 \n",
      "Target [No] and group [Hispanic]: 0.209 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.327 \n",
      "{'White': 0.306, 'Black': 0.323, 'Other': 0.26, 'Asian': 0.18, 'Hispanic': 0.209, 'American Indian/Alaskan Native': 0.327}\n",
      "Class differences std: 0.05662375826453062\n",
      "Target [Yes] and group [White]: 0.768 \n",
      "Target [Yes] and group [Black]: 0.776 \n",
      "Target [Yes] and group [Other]: 0.681 \n",
      "Target [Yes] and group [Asian]: 0.786 \n",
      "Target [Yes] and group [Hispanic]: 0.674 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.756 \n",
      "{'White': 0.768, 'Black': 0.776, 'Other': 0.681, 'Asian': 0.786, 'Hispanic': 0.674, 'American Indian/Alaskan Native': 0.756}\n",
      "Class differences std: 0.04525636848984779\n",
      "Total class difference: 0.10188012675437841\n",
      "Total class difference: 0.10188012675437841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:11,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:46,201]\u001b[0m Trial 2 finished with values: [0.10188012675437841, 0.31055406346364267] and parameters: {'criterion': 'gini', 'max_depth': 7, 'min_samples_split': 0.002607186152009599, 'min_samples_leaf': 0.00901664241812518}. \u001b[0m\n",
      "Target [No] and group [White]: 0.331 \n",
      "Target [No] and group [Black]: 0.346 \n",
      "Target [No] and group [Other]: 0.287 \n",
      "Target [No] and group [Asian]: 0.191 \n",
      "Target [No] and group [Hispanic]: 0.229 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.368 \n",
      "{'White': 0.331, 'Black': 0.346, 'Other': 0.287, 'Asian': 0.191, 'Hispanic': 0.229, 'American Indian/Alaskan Native': 0.368}\n",
      "Class differences std: 0.06378087487640789\n",
      "Target [Yes] and group [White]: 0.804 \n",
      "Target [Yes] and group [Black]: 0.806 \n",
      "Target [Yes] and group [Other]: 0.716 \n",
      "Target [Yes] and group [Asian]: 0.81 \n",
      "Target [Yes] and group [Hispanic]: 0.683 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.811 \n",
      "{'White': 0.804, 'Black': 0.806, 'Other': 0.716, 'Asian': 0.81, 'Hispanic': 0.683, 'American Indian/Alaskan Native': 0.811}\n",
      "Class differences std: 0.051963662517399825\n",
      "Total class difference: 0.11574453739380772\n",
      "Total class difference: 0.11574453739380772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:06<00:09,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:47,849]\u001b[0m Trial 3 finished with values: [0.11574453739380772, 0.30579635443512493] and parameters: {'criterion': 'gini', 'max_depth': 23, 'min_samples_split': 0.002822126005113079, 'min_samples_leaf': 0.008652830657081662}. \u001b[0m\n",
      "Target [No] and group [White]: 0.331 \n",
      "Target [No] and group [Black]: 0.346 \n",
      "Target [No] and group [Other]: 0.287 \n",
      "Target [No] and group [Asian]: 0.191 \n",
      "Target [No] and group [Hispanic]: 0.229 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.368 \n",
      "{'White': 0.331, 'Black': 0.346, 'Other': 0.287, 'Asian': 0.191, 'Hispanic': 0.229, 'American Indian/Alaskan Native': 0.368}\n",
      "Class differences std: 0.06378087487640789\n",
      "Target [Yes] and group [White]: 0.804 \n",
      "Target [Yes] and group [Black]: 0.806 \n",
      "Target [Yes] and group [Other]: 0.716 \n",
      "Target [Yes] and group [Asian]: 0.81 \n",
      "Target [Yes] and group [Hispanic]: 0.683 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.811 \n",
      "{'White': 0.804, 'Black': 0.806, 'Other': 0.716, 'Asian': 0.81, 'Hispanic': 0.683, 'American Indian/Alaskan Native': 0.811}\n",
      "Class differences std: 0.051963662517399825\n",
      "Total class difference: 0.11574453739380772\n",
      "Total class difference: 0.11574453739380772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:08<00:07,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:49,428]\u001b[0m Trial 4 finished with values: [0.11574453739380772, 0.30579635443512493] and parameters: {'criterion': 'gini', 'max_depth': 13, 'min_samples_split': 0.003985694243953256, 'min_samples_leaf': 0.008620218004672955}. \u001b[0m\n",
      "Target [No] and group [White]: 0.311 \n",
      "Target [No] and group [Black]: 0.333 \n",
      "Target [No] and group [Other]: 0.267 \n",
      "Target [No] and group [Asian]: 0.176 \n",
      "Target [No] and group [Hispanic]: 0.227 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.342 \n",
      "{'White': 0.311, 'Black': 0.333, 'Other': 0.267, 'Asian': 0.176, 'Hispanic': 0.227, 'American Indian/Alaskan Native': 0.342}\n",
      "Class differences std: 0.05959865770300537\n",
      "Target [Yes] and group [White]: 0.82 \n",
      "Target [Yes] and group [Black]: 0.825 \n",
      "Target [Yes] and group [Other]: 0.787 \n",
      "Target [Yes] and group [Asian]: 0.786 \n",
      "Target [Yes] and group [Hispanic]: 0.723 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.811 \n",
      "{'White': 0.82, 'Black': 0.825, 'Other': 0.787, 'Asian': 0.786, 'Hispanic': 0.723, 'American Indian/Alaskan Native': 0.811}\n",
      "Class differences std: 0.03429285639896449\n",
      "Total class difference: 0.09389151410196986\n",
      "Total class difference: 0.09389151410196986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:09<00:06,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:51,189]\u001b[0m Trial 5 finished with values: [0.09389151410196986, 0.32397030999854465] and parameters: {'criterion': 'gini', 'max_depth': 15, 'min_samples_split': 0.005823028851292857, 'min_samples_leaf': 0.000844805460130891}. \u001b[0m\n",
      "Target [No] and group [White]: 0.323 \n",
      "Target [No] and group [Black]: 0.332 \n",
      "Target [No] and group [Other]: 0.269 \n",
      "Target [No] and group [Asian]: 0.177 \n",
      "Target [No] and group [Hispanic]: 0.218 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.338 \n",
      "{'White': 0.323, 'Black': 0.332, 'Other': 0.269, 'Asian': 0.177, 'Hispanic': 0.218, 'American Indian/Alaskan Native': 0.338}\n",
      "Class differences std: 0.061104873419028996\n",
      "Target [Yes] and group [White]: 0.813 \n",
      "Target [Yes] and group [Black]: 0.795 \n",
      "Target [Yes] and group [Other]: 0.759 \n",
      "Target [Yes] and group [Asian]: 0.786 \n",
      "Target [Yes] and group [Hispanic]: 0.696 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.778 \n",
      "{'White': 0.813, 'Black': 0.795, 'Other': 0.759, 'Asian': 0.786, 'Hispanic': 0.696, 'American Indian/Alaskan Native': 0.778}\n",
      "Class differences std: 0.03737832110848332\n",
      "Total class difference: 0.09848319452751231\n",
      "Total class difference: 0.09848319452751231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:11<00:05,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:52,902]\u001b[0m Trial 6 finished with values: [0.09848319452751231, 0.31453807591310573] and parameters: {'criterion': 'entropy', 'max_depth': 17, 'min_samples_split': 0.007767384187416649, 'min_samples_leaf': 0.00421397225896351}. \u001b[0m\n",
      "Target [No] and group [White]: 0.33 \n",
      "Target [No] and group [Black]: 0.321 \n",
      "Target [No] and group [Other]: 0.274 \n",
      "Target [No] and group [Asian]: 0.175 \n",
      "Target [No] and group [Hispanic]: 0.215 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.352 \n",
      "{'White': 0.33, 'Black': 0.321, 'Other': 0.274, 'Asian': 0.175, 'Hispanic': 0.215, 'American Indian/Alaskan Native': 0.352}\n",
      "Class differences std: 0.06406095187831318\n",
      "Target [Yes] and group [White]: 0.799 \n",
      "Target [Yes] and group [Black]: 0.749 \n",
      "Target [Yes] and group [Other]: 0.681 \n",
      "Target [Yes] and group [Asian]: 0.786 \n",
      "Target [Yes] and group [Hispanic]: 0.656 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.811 \n",
      "{'White': 0.799, 'Black': 0.749, 'Other': 0.681, 'Asian': 0.786, 'Hispanic': 0.656, 'American Indian/Alaskan Native': 0.811}\n",
      "Class differences std: 0.059107247158590184\n",
      "Total class difference: 0.12316819903690336\n",
      "Total class difference: 0.12316819903690336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:13<00:03,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:54,501]\u001b[0m Trial 7 finished with values: [0.12316819903690336, 0.30556082748149554] and parameters: {'criterion': 'entropy', 'max_depth': 27, 'min_samples_split': 0.0027131624511479743, 'min_samples_leaf': 0.009848968399388196}. \u001b[0m\n",
      "Target [No] and group [White]: 0.319 \n",
      "Target [No] and group [Black]: 0.34 \n",
      "Target [No] and group [Other]: 0.278 \n",
      "Target [No] and group [Asian]: 0.186 \n",
      "Target [No] and group [Hispanic]: 0.222 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.361 \n",
      "{'White': 0.319, 'Black': 0.34, 'Other': 0.278, 'Asian': 0.186, 'Hispanic': 0.222, 'American Indian/Alaskan Native': 0.361}\n",
      "Class differences std: 0.06294618512842715\n",
      "Target [Yes] and group [White]: 0.793 \n",
      "Target [Yes] and group [Black]: 0.802 \n",
      "Target [Yes] and group [Other]: 0.695 \n",
      "Target [Yes] and group [Asian]: 0.81 \n",
      "Target [Yes] and group [Hispanic]: 0.683 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.789 \n",
      "{'White': 0.793, 'Black': 0.802, 'Other': 0.695, 'Asian': 0.81, 'Hispanic': 0.683, 'American Indian/Alaskan Native': 0.789}\n",
      "Class differences std: 0.05216001022494789\n",
      "Total class difference: 0.11510619535337505\n",
      "Total class difference: 0.11510619535337505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:14<00:01,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:56,000]\u001b[0m Trial 8 finished with values: [0.11510619535337505, 0.3091328147507923] and parameters: {'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 0.00031897135755917165, 'min_samples_leaf': 0.008464050882476109}. \u001b[0m\n",
      "Target [No] and group [White]: 0.331 \n",
      "Target [No] and group [Black]: 0.346 \n",
      "Target [No] and group [Other]: 0.287 \n",
      "Target [No] and group [Asian]: 0.191 \n",
      "Target [No] and group [Hispanic]: 0.229 \n",
      "Target [No] and group [American Indian/Alaskan Native]: 0.368 \n",
      "{'White': 0.331, 'Black': 0.346, 'Other': 0.287, 'Asian': 0.191, 'Hispanic': 0.229, 'American Indian/Alaskan Native': 0.368}\n",
      "Class differences std: 0.06378087487640789\n",
      "Target [Yes] and group [White]: 0.804 \n",
      "Target [Yes] and group [Black]: 0.806 \n",
      "Target [Yes] and group [Other]: 0.716 \n",
      "Target [Yes] and group [Asian]: 0.81 \n",
      "Target [Yes] and group [Hispanic]: 0.683 \n",
      "Target [Yes] and group [American Indian/Alaskan Native]: 0.811 \n",
      "{'White': 0.804, 'Black': 0.806, 'Other': 0.716, 'Asian': 0.81, 'Hispanic': 0.683, 'American Indian/Alaskan Native': 0.811}\n",
      "Class differences std: 0.051963662517399825\n",
      "Total class difference: 0.11574453739380772\n",
      "Total class difference: 0.11574453739380772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-22 13:41:57,622]\u001b[0m Trial 9 finished with values: [0.11574453739380772, 0.30579635443512493] and parameters: {'criterion': 'gini', 'max_depth': 25, 'min_samples_split': 0.0038181379959557047, 'min_samples_leaf': 0.008371904399506127}. \u001b[0m\n",
      "[FrozenTrial(number=5, values=[0.09389151410196986, 0.32397030999854465], datetime_start=datetime.datetime(2022, 5, 22, 13, 41, 49, 430214), datetime_complete=datetime.datetime(2022, 5, 22, 13, 41, 51, 189863), params={'criterion': 'gini', 'max_depth': 15, 'min_samples_split': 0.005823028851292857, 'min_samples_leaf': 0.000844805460130891}, distributions={'criterion': CategoricalDistribution(choices=('gini', 'entropy')), 'max_depth': IntUniformDistribution(high=29, low=5, step=2), 'min_samples_split': UniformDistribution(high=0.01, low=1e-05), 'min_samples_leaf': UniformDistribution(high=0.01, low=1e-05)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=5, state=TrialState.COMPLETE, value=None)]\n",
      "Counter({'No': 47632, 'Yes': 337})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.92      1.00      0.96     43863\n",
      "         Yes       0.58      0.05      0.09      4106\n",
      "\n",
      "    accuracy                           0.92     47969\n",
      "   macro avg       0.75      0.52      0.52     47969\n",
      "weighted avg       0.89      0.92      0.88     47969\n",
      "\n",
      "Total class difference: 0.009591829538468145\n",
      "(0.009591829538468145, {'No': {'White': 0.003, 'Black': 0.005, 'Other': 0.004, 'Asian': 0.003, 'Hispanic': 0.002, 'American Indian/Alaskan Native': 0.003}, 'Yes': {'White': 0.045, 'Black': 0.053, 'Other': 0.057, 'Asian': 0.071, 'Hispanic': 0.067, 'American Indian/Alaskan Native': 0.056}})\n"
     ]
    }
   ],
   "source": [
    "# Run optimization\n",
    "sampler = optuna.samplers.TPESampler() \n",
    "\n",
    "study = optuna.create_study(directions=[\"minimize\", \"maximize\"],\n",
    "#study = optuna.create_study(direction=\"maximize\",\n",
    "                            sampler=sampler,\n",
    "                            pruner=optuna.pruners.MedianPruner(\n",
    "                                n_startup_trials=2, n_warmup_steps=5, interval_steps=3\n",
    "                                ),\n",
    "                            )\n",
    "\n",
    "# Scaling\n",
    "# Scale continuous variables\n",
    "scaler = ColumnTransformer(\n",
    "    [('scaler', StandardScaler(), continuos_features)], remainder='passthrough')\n",
    "X_train = scaler.fit_transform(train_processed)\n",
    "X_val = scaler.transform(val_processed)\n",
    "\n",
    "ros = RandomOverSampler(random_state =42)\n",
    "X_train_resampled , y_train_resampled , = ros.fit_resample(X_train , y_train)\n",
    "# Define objective\n",
    "#objective = Objective(X_train, X_val, y_train, y_val, val_race, equailized_odds)\n",
    "objective = Objective(X_train_resampled, X_val, y_train_resampled, y_val, val_race, equailized_odds)\n",
    "\n",
    "# Make a study to optimize the objective.\n",
    "study.optimize(objective, n_trials=10, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print(study.best_trials)\n",
    "\n",
    "#best_params = study.best_params\n",
    "\n",
    "scaler = ColumnTransformer(\n",
    "    [('scaler', StandardScaler(), continuos_features)], remainder='passthrough')\n",
    "    \n",
    "whitebox_model = Pipeline(steps=[('scaler', scaler), ('clf', DecisionTreeClassifier(**best_params, random_state = 42))])\n",
    "# whitebox_model = Pipeline(steps=[('scaler', scaler), ('clf', DecisionTreeClassifier(random_state = 42))])\n",
    "\n",
    "whitebox_model.fit(train_processed, y_train)\n",
    "y_pred_whitebox = whitebox_model.predict(val_processed)\n",
    "y_pred_proba_whitebox = whitebox_model.predict_proba(val_processed)\n",
    "\n",
    "print(Counter(y_pred_whitebox))\n",
    "\n",
    "print(classification_report(y_val, y_pred_whitebox))\n",
    "\n",
    "print(equailized_odds(y_pred_whitebox, val_race, y_val, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'White': 36714,\n",
       "         'Black': 3401,\n",
       "         'Other': 1697,\n",
       "         'Asian': 1189,\n",
       "         'Hispanic': 4139,\n",
       "         'American Indian/Alaskan Native': 829})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(val_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "# Checking for correlation between variables\n",
    "from dython import nominal\n",
    "\n",
    "data_test = train_original.copy()\n",
    "\n",
    "# Encode the object as an enumerated type or categorical variable.\n",
    "data_test[categorical_features] = data_test[categorical_features].apply(lambda x : pd.factorize(x)[0])\n",
    "nominal.associations(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train whitebox model\n",
    "\n",
    "# Scale continuous variables\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaler = ColumnTransformer([('scaler', StandardScaler(), continuos_features)], remainder='passthrough')\n",
    "\n",
    "# Whitebox model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaler = ColumnTransformer(\n",
    "    [('scaler', StandardScaler(), continuos_features)], remainder='passthrough')\n",
    "    \n",
    "# whitebox_model = Pipeline(steps=[('scaler', scaler), ('clf', DecisionTreeClassifier(min_samples_split = 0.01, min_samples_leaf= 0.01, max_features=\"auto\", max_depth = 5, criterion = \"gini\", random_state = 42))])\n",
    "whitebox_model = Pipeline(steps=[('scaler', scaler), ('clf', DecisionTreeClassifier(random_state = 42))])\n",
    "\n",
    "whitebox_model.fit(train_processed, y_train)\n",
    "y_pred_whitebox = whitebox_model.predict(val_processed)\n",
    "y_pred_proba_whitebox = whitebox_model.predict_proba(val_processed)\n",
    "\n",
    "print(Counter(y_pred_whitebox))\n",
    "\n",
    "print(classification_report(y_val, y_pred_whitebox))\n",
    "\n",
    "print(equailized_odds(y_pred_whitebox, val_race, y_val, verbose=False))\n",
    "\n",
    "# # plot tree\n",
    "# plt.figure(figsize=(25,20))  # set plot size (denoted in inches)\n",
    "# tree.plot_tree(whitebox_model['clf'], fontsize=9, feature_names=df_processed.columns)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4168ca1474844f3f5fca761e685cf164ea20722acc5773b61bcc07160dedfd10"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
